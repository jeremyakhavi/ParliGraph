{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_references(doc):\n",
    "    token_mention_mapper = {}\n",
    "    output_str = \"\"\n",
    "    clusters = [\n",
    "        val for key, val in doc.spans.items() if key.startswith(\"coref_cluster\")\n",
    "    ]\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        first_mention = cluster[0]\n",
    "        # replace mentions of an entity with the text of the first mention\n",
    "        for mention_span in list(cluster)[1:]:\n",
    "            # key is the index of the mention in the original string\n",
    "            # value is the first mention + the whitespace of the current mention\n",
    "            token_mention_mapper[mention_span[0].idx] = first_mention.text + mention_span[0].whitespace_\n",
    "            \n",
    "            # if there are any other words in the mention, then replace them with the empty string\n",
    "            for token in mention_span[1:]:\n",
    "                token_mention_mapper[token.idx] = \"\"\n",
    "    \n",
    "    # loop through all tokens in original string, if the word/phrase has been identified as an entity\n",
    "    # then replace it with the value given above\n",
    "    for token in doc:\n",
    "        if token.idx in token_mention_mapper:\n",
    "            output_str += token_mention_mapper[token.idx]\n",
    "        else:\n",
    "            output_str += token.text + token.whitespace_\n",
    "    return output_str\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5fde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from string import punctuation\n",
    "import json\n",
    "\n",
    "label_map = {\n",
    "        \"person\": \"Person\",\n",
    "        \"school\": \"School\",\n",
    "        \"higher education institution\": \"University\",\n",
    "        \"city/town\": \"Location\",\n",
    "        \"country\": \"Location\",\n",
    "        \"geographic region\": \"Location\",\n",
    "        \"location\": \"Location\",\n",
    "        \"political party\": \"Party\",\n",
    "        \"company\": \"Organisation\",\n",
    "        \"business\": \"Organisation\",\n",
    "        \"organization\": \"Organisation\",\n",
    "    }\n",
    "\n",
    "def get_label(annotation_classes):\n",
    "    for wiki_class in annotation_classes:\n",
    "        label = label_map.get(wiki_class['enLabel'])\n",
    "        if label:\n",
    "            return label\n",
    "    return None\n",
    "\n",
    "'''\n",
    "pageRankSqThreshold prunes annotations based on their page ranks\n",
    "applyPageRankSqThreshold discards all annotations that have been pruned\n",
    "wikiDataClasses returns wikidata list (concept ID, concept name) for all classes that the concept belongs to\n",
    "maxMentionEntropy ignore highly ambiguous mentions\n",
    "'''\n",
    "\n",
    "def entity_naming(text, threshold=0.8):\n",
    "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
    "    # Prepare the URL.\n",
    "    #TODO: move API key to .env file\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"lang\": \"en\",\n",
    "        \"userKey\": \"bknexcqfanbxjxnubamwxgdnzybwyz\",\n",
    "        # prune annotations based on page rank\n",
    "        \"pageRankSqThreshold\": threshold,\n",
    "        # discard all annotations that have been pruned\n",
    "        \"applyPageRankSqThreshold\": \"true\",\n",
    "        \"support\": \"true\",\n",
    "        \"minLinkFrequency\": \"true\",\n",
    "        \"ranges\": \"false\",\n",
    "        \"includeCosines\": \"false\",\n",
    "        # ignore ambiguous mentions\n",
    "        \"maxMentionEntropy\": \"3\"\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    response = requests.post(url, data=data, headers=headers, timeout=60)\n",
    "    # call wikifier api and read the response\n",
    "    if response.status_code == 200:\n",
    "        response = json.loads(response.content.decode('utf8'))\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} when using wikifier API\")\n",
    "        return None\n",
    "    # output the annotations\n",
    "    results = []\n",
    "    for annotation in response['annotations']:\n",
    "        # only get desired annotations\n",
    "        if ('wikiDataClasses' in annotation):\n",
    "            # Get entity label\n",
    "            label = get_label(annotation['wikiDataClasses'])\n",
    "            # If label is returned then add to results dict\n",
    "            if label is not None:\n",
    "                results.append({'title': annotation['title'], 'label': label,\n",
    "                                'characters': [(data['chFrom'], data['chTo']) for data in annotation['support']]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da63f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opennre\n",
    "import spacy\n",
    "\n",
    "def nlp_pipeline(doc, mp_name):\n",
    "    relation_model = opennre.get_model('tacred_bertentity_softmax')\n",
    "    tokeniser = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    tokeniser.add_pipe('sentencizer')\n",
    "    \n",
    "    relation_threshold = 0.9\n",
    "\n",
    "    resolved_txt = resolve_references(doc)\n",
    "    \n",
    "    entities_dict = {}\n",
    "    relation_dict_list = []\n",
    "\n",
    "    tokenised_txt = tokeniser(resolved_txt)\n",
    "    for sentence in tokenised_txt.sents:\n",
    "        # strip punctuation\n",
    "        sentence = ''.join(char for char in sentence.text if char not in punctuation)\n",
    "        entities = entity_naming(sentence)\n",
    "\n",
    "        # should only be 1 entity with mp_name as title, so return characters for that entry, return empty list if not found\n",
    "        mp_positions = next((entity['characters'] for entity in entities if entity['title'] == mp_name), [])\n",
    "        # check that character indexes match mp_name in sentence\n",
    "        valid_mp_pos = [t for t in mp_positions if sentence[t[0]:t[1]+1] == mp_name]\n",
    "\n",
    "        for entity in entities:\n",
    "            # don't want target to be the MP themselves, or their political party\n",
    "            if entity['title'] != mp_name and entity['label'] != 'Party':\n",
    "                    for mp_pos in valid_mp_pos:\n",
    "                        for target in entity['characters']:\n",
    "                                data = relation_model.infer(\n",
    "                                    {'text': sentence,\n",
    "                                        'h': {'pos': [mp_pos[0], mp_pos[1] + 1]},\n",
    "                                        't': {'pos': [target[0], target[1] + 1]}}\n",
    "                                )\n",
    "                                if data[1] > relation_threshold and data[0] != 'NA':\n",
    "                                    relation_dict_list.append(\n",
    "                                    {'source': mp_name, 'target': entity['title'], 'type': data[0]})\n",
    "                                    entities_dict[entity['title']] = entity['label']\n",
    "    # deduplicate a list of dictionaries by converting them to frozensets as keys in a new dictionary, then extracting unique values\n",
    "    unique_relations = list({frozenset(d.items()): d for d in relation_dict_list}.values())\n",
    "\n",
    "    return entities_dict, unique_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_rel_work(tx, source_name, target_label, target_name, relation_type):\n",
    "    return tx.run(f\"MATCH (m:MP {{name: $source_name}}) \\\n",
    "                  MERGE (t:{target_label} {{name: $target_name}}) \\\n",
    "                  MERGE (m)-[:{relation_type}]->(t) \\\n",
    "                  RETURN m, t\",\n",
    "                  source_name=source_name, target_name=target_name).single()\n",
    "\n",
    "def create_new_rel(driver, source_name, target_label, target_name, relation_type):\n",
    "    session = driver.session()\n",
    "\n",
    "    record = session.execute_write(create_new_rel_work,\n",
    "                                   source_name=source_name, target_label=target_label,\n",
    "                                   target_name=target_name, relation_type=relation_type)\n",
    "    \n",
    "    target = record['t']\n",
    "    print(target)\n",
    "\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feef4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import Database\n",
    "import os\n",
    "import pprint\n",
    "import wikipedia\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_coreference_web_trf\")\n",
    "\n",
    "mp_name = 'Emily Thornberry'\n",
    "\n",
    "wiki = wikipedia.page(mp_name)\n",
    "text = wiki.content\n",
    "# prepend wiki text with MP name as in graph as sometimes the wikipedia articles include middle names\n",
    "text = f'{mp_name}. ' + text\n",
    "# only interested in wikipedia content before the references section\n",
    "text = text.split(\"== References ==\")[0]\n",
    "\n",
    "doc = nlp(text)\n",
    "resolve_references(doc)\n",
    "\n",
    "for i, pipe in enumerate(nlp.pipe_names):\n",
    "    print(f\"{i}, {pipe}\")\n",
    "\n",
    "entities, relations = nlp_pipeline(doc, mp_name)\n",
    "print(pprint.pprint(entities))\n",
    "\n",
    "for r in relations:\n",
    "    print(pprint.pprint(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8075fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = Database.init_driver(os.getenv(\"NEO4J_URI\"), os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "for trip in relations:\n",
    "    target_name = trip['target']\n",
    "    target_label = entities[target_name]\n",
    "    # strip and capitalise relation type to only include type, so 'EMPLOYEE_OF' instead of 'per:employee_of'\n",
    "    relation_type = trip['type'].split(':')[1].upper()\n",
    "    create_new_rel(driver, mp_name, target_label, target_name, relation_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0356a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e975bf0b5e4cb213377fc8c5c8cc667efc04f27b55ac95fed51774df1495ae59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
