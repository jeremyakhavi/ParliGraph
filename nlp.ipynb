{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_references(doc):\n",
    "    token_mention_mapper = {}\n",
    "    output_str = \"\"\n",
    "    clusters = [\n",
    "        val for key, val in doc.spans.items() if key.startswith(\"coref_cluster\")\n",
    "    ]\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        first_mention = cluster[0]\n",
    "        # replace mentions of an entity with the text of the first mention\n",
    "        for mention_span in list(cluster)[1:]:\n",
    "            # key is the index of the mention in the original string\n",
    "            # value is the first mention + the whitespace of the current mention\n",
    "            token_mention_mapper[mention_span[0].idx] = first_mention.text + mention_span[0].whitespace_\n",
    "            \n",
    "            # if there are any other words in the mention, then replace them with the empty string\n",
    "            for token in mention_span[1:]:\n",
    "                token_mention_mapper[token.idx] = \"\"\n",
    "    \n",
    "    # loop through all tokens in original string, if the word/phrase has been identified as an entity\n",
    "    # then replace it with the value given above\n",
    "    for token in doc:\n",
    "        if token.idx in token_mention_mapper:\n",
    "            output_str += token_mention_mapper[token.idx]\n",
    "        else:\n",
    "            output_str += token.text + token.whitespace_\n",
    "    return output_str\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f5fde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from string import punctuation\n",
    "import json\n",
    "\n",
    "label_map = {\n",
    "        \"person\": \"Person\",\n",
    "        \"school\": \"School\",\n",
    "        \"higher education institution\": \"University\",\n",
    "        \"city/town\": \"Location\",\n",
    "        \"country\": \"Location\",\n",
    "        \"geographic region\": \"Location\",\n",
    "        \"location\": \"Location\",\n",
    "        \"political party\": \"Party\",\n",
    "        \"company\": \"Organisation\",\n",
    "        \"business\": \"Organisation\",\n",
    "        \"organization\": \"Organisation\",\n",
    "    }\n",
    "\n",
    "def get_label(annotation_classes):\n",
    "    for wiki_class in annotation_classes:\n",
    "        label = label_map.get(wiki_class['enLabel'])\n",
    "        if label:\n",
    "            return label\n",
    "    return None\n",
    "\n",
    "'''\n",
    "pageRankSqThreshold prunes annotations based on their page ranks\n",
    "applyPageRankSqThreshold discards all annotations that have been pruned\n",
    "wikiDataClasses returns wikidata list (concept ID, concept name) for all classes that the concept belongs to\n",
    "maxMentionEntropy ignore highly ambiguous mentions\n",
    "'''\n",
    "\n",
    "def entity_naming(text, threshold=0.8):\n",
    "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
    "    # Prepare the URL.\n",
    "    #TODO: move API key to .env file\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"lang\": \"en\",\n",
    "        \"userKey\": \"bknexcqfanbxjxnubamwxgdnzybwyz\",\n",
    "        # prune annotations based on page rank\n",
    "        \"pageRankSqThreshold\": threshold,\n",
    "        # discard all annotations that have been pruned\n",
    "        \"applyPageRankSqThreshold\": \"true\",\n",
    "        \"support\": \"true\",\n",
    "        \"minLinkFrequency\": \"true\",\n",
    "        \"ranges\": \"false\",\n",
    "        \"includeCosines\": \"false\",\n",
    "        # ignore ambiguous mentions\n",
    "        \"maxMentionEntropy\": \"3\"\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    response = requests.post(url, data=data, headers=headers, timeout=60)\n",
    "    # call wikifier api and read the response\n",
    "    if response.status_code == 200:\n",
    "        response = json.loads(response.content.decode('utf8'))\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} when using wikifier API\")\n",
    "        return None\n",
    "    # output the annotations\n",
    "    results = []\n",
    "    for annotation in response['annotations']:\n",
    "        # only get desired annotations\n",
    "        if ('wikiDataClasses' in annotation):\n",
    "            # Get entity label\n",
    "            label = get_label(annotation['wikiDataClasses'])\n",
    "            # If label is returned then add to results dict\n",
    "            if label is not None:\n",
    "                results.append({'title': annotation['title'], 'label': label,\n",
    "                                'characters': [(data['chFrom'], data['chTo']) for data in annotation['support']]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8da63f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opennre\n",
    "\n",
    "def nlp_pipeline(doc, mp_name):\n",
    "    relation_model = opennre.get_model('tacred_bertentity_softmax')\n",
    "    tokeniser = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    tokeniser.add_pipe('sentencizer')\n",
    "    \n",
    "    relation_threshold = 0.9\n",
    "\n",
    "    resolved_txt = resolve_references(doc)\n",
    "    \n",
    "    entities_dict = {}\n",
    "    relation_dict_list = []\n",
    "\n",
    "    tokenised_txt = tokeniser(resolved_txt)\n",
    "    for sentence in tokenised_txt.sents:\n",
    "        # strip punctuation\n",
    "        sentence = ''.join(char for char in sentence.text if char not in punctuation)\n",
    "        entities = entity_naming(sentence)\n",
    "\n",
    "        # should only be 1 entity with mp_name as title, so return characters for that entry, return empty list if not found\n",
    "        mp_positions = next((entity['characters'] for entity in entities if entity['title'] == mp_name), [])\n",
    "        # check that character indexes match mp_name in sentence\n",
    "        valid_mp_pos = [t for t in mp_positions if sentence[t[0]:t[1]+1] == mp_name]\n",
    "\n",
    "        for entity in entities:\n",
    "            if entity['title'] != mp_name:\n",
    "                    for mp_pos in valid_mp_pos:\n",
    "                        for target in entity['characters']:\n",
    "                                data = relation_model.infer(\n",
    "                                    {'text': sentence,\n",
    "                                        'h': {'pos': [mp_pos[0], mp_pos[1] + 1]},\n",
    "                                        't': {'pos': [target[0], target[1] + 1]}}\n",
    "                                )\n",
    "                                if data[1] > relation_threshold and data[0] != 'NA':\n",
    "                                    relation_dict_list.append(\n",
    "                                    {'source': mp_name, 'target': entity['title'], 'type': data[0]})\n",
    "                                    entities_dict[entity['title']] = entity['label']\n",
    "    # deduplicate a list of dictionaries by converting them to frozensets as keys in a new dictionary, then extracting unique values\n",
    "    unique_relations = list({frozenset(d.items()): d for d in relation_dict_list}.values())\n",
    "\n",
    "    return entities_dict, unique_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_rel_work(tx, source_name, target_label, target_name, relation_type):\n",
    "    return tx.run(f\"MATCH (m:MP {{name: $source_name}}) \\\n",
    "                  MERGE (t:{target_label} {{name: $target_name}}) \\\n",
    "                  MERGE (m)-[:{relation_type}]->(t) \\\n",
    "                  RETURN m, t\",\n",
    "                  source_name=source_name, target_name=target_name).single()\n",
    "\n",
    "def create_new_rel(driver, source_name, target_label, target_name, relation_type):\n",
    "    session = driver.session()\n",
    "\n",
    "    record = session.execute_write(create_new_rel_work,\n",
    "                                   source_name=source_name, target_label=target_label,\n",
    "                                   target_name=target_name, relation_type=relation_type)\n",
    "    \n",
    "    target = record['t']\n",
    "    print(target)\n",
    "\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6feef4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 18:21:14,438 - root - INFO - Loading BERT pre-trained checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, sentencizer\n",
      "1, transformer\n",
      "2, coref\n",
      "3, span_resolver\n",
      "4, span_cleaner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/fyjca2/.opennre/pretrain/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Islington South and Finsbury (UK Parliament constituency)\n",
      "Chars:  [(82, 96), (82, 109)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(2, 27), (12, 27), (16, 21), (16, 27)]\n",
      "Title:  England and Wales\n",
      "Chars:  [(88, 104)]\n",
      "Title:  Department for International Trade\n",
      "Chars:  [(215, 219)]\n",
      "Title:  Surrey\n",
      "Chars:  [(294, 299)]\n",
      "Title:  University of Kent\n",
      "Chars:  [(26, 43)]\n",
      "Title:  Transport and General Workers' Union\n",
      "Chars:  [(141, 175)]\n",
      "Title:  England\n",
      "Chars:  [(100, 106), (242, 248)]\n",
      "Title:  England and Wales\n",
      "Chars:  [(100, 116)]\n",
      "Title:  Ed Miliband\n",
      "Chars:  [(121, 122)]\n",
      "Title:  House of Commons of the United Kingdom\n",
      "Chars:  [(231, 235)]\n",
      "Title:  Jeremy Corbyn\n",
      "Chars:  [(6, 18), (13, 18)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(33, 48), (37, 42), (37, 48)]\n",
      "Title:  Department for Work and Pensions\n",
      "Chars:  [(101, 139), (108, 139), (126, 139), (130, 139)]\n",
      "Title:  Jeremy Corbyn\n",
      "Chars:  [(44, 56), (51, 56)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(61, 93), (71, 86), (75, 80), (75, 86), (100, 105), (100, 111), (166, 171), (166, 177)]\n",
      "Title:  England and Wales\n",
      "Chars:  [(59, 75)]\n",
      "Title:  Cedric Thornberry\n",
      "Chars:  [(71, 87)]\n",
      "Title:  London School of Economics\n",
      "Chars:  [(131, 160), (135, 160)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(26, 41), (30, 35), (30, 41), (30, 52), (143, 158), (147, 152), (147, 158)]\n",
      "Title:  Guildford\n",
      "Chars:  [(86, 94), (174, 182)]\n",
      "Title:  University of Kent\n",
      "Chars:  [(45, 62)]\n",
      "Title:  Michael Mansfield\n",
      "Chars:  [(138, 154)]\n",
      "Title:  Transport and General Workers' Union\n",
      "Chars:  [(28, 62)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(103, 118), (107, 112), (107, 118)]\n",
      "Title:  Julian Brazier\n",
      "Chars:  [(191, 204), (198, 204)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(94, 109), (98, 103), (98, 109), (167, 172), (167, 178)]\n",
      "Title:  Islington South and Finsbury (UK Parliament constituency)\n",
      "Chars:  [(125, 139), (125, 152)]\n",
      "Title:  Nick Smith (British politician)\n",
      "Chars:  [(0, 9)]\n",
      "Title:  House of Commons of the United Kingdom\n",
      "Chars:  [(134, 149)]\n",
      "Title:  Philip Mawer\n",
      "Chars:  [(88, 99)]\n",
      "Title:  Housing Corporation\n",
      "Chars:  [(184, 194)]\n",
      "Title:  Friends of the Earth\n",
      "Chars:  [(54, 73)]\n",
      "Title:  World Wide Fund for Nature\n",
      "Chars:  [(79, 93), (79, 104), (106, 108)]\n",
      "Title:  Department of Energy and Climate Change\n",
      "Chars:  [(68, 87), (68, 106), (82, 106), (93, 106)]\n",
      "Title:  Joan Ruddock\n",
      "Chars:  [(165, 176), (170, 176)]\n",
      "Title:  Ed Miliband\n",
      "Chars:  [(182, 183), (182, 192), (185, 192)]\n",
      "Title:  Ed Miliband\n",
      "Chars:  [(21, 22), (21, 31), (24, 31)]\n",
      "Title:  Islington\n",
      "Chars:  [(94, 102), (153, 161)]\n",
      "Title:  Islington South and Finsbury (UK Parliament constituency)\n",
      "Chars:  [(94, 108), (94, 121), (153, 167)]\n",
      "Title:  Department of Energy and Climate Change\n",
      "Chars:  [(226, 245), (226, 264), (240, 264), (251, 264)]\n",
      "Title:  Department of Energy and Climate Change\n",
      "Chars:  [(27, 46), (27, 65), (41, 65), (52, 65)]\n",
      "Title:  Charles Hendry\n",
      "Chars:  [(93, 106), (101, 106)]\n",
      "Title:  John Healey (politician)\n",
      "Chars:  [(108, 118)]\n",
      "Title:  Government of the United Kingdom\n",
      "Chars:  [(66, 76), (182, 192)]\n",
      "Title:  Dominic Grieve\n",
      "Chars:  [(38, 51)]\n",
      "Title:  Bernie Ecclestone\n",
      "Chars:  [(222, 238), (229, 238)]\n",
      "Title:  David Cameron\n",
      "Chars:  [(51, 63), (85, 97)]\n",
      "Title:  Islington\n",
      "Chars:  [(127, 135), (253, 261)]\n",
      "Title:  Rochester and Strood (UK Parliament constituency)\n",
      "Chars:  [(121, 140)]\n",
      "Title:  Twitter\n",
      "Chars:  [(65, 72)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(42, 57), (46, 51), (46, 57), (46, 61), (388, 403), (392, 397), (392, 403)]\n",
      "Title:  Ed Miliband\n",
      "Chars:  [(80, 81), (80, 90), (83, 90)]\n",
      "Title:  Chris Bryant\n",
      "Chars:  [(214, 225)]\n",
      "Title:  Simon Danczuk\n",
      "Chars:  [(360, 372), (366, 372)]\n",
      "Title:  London\n",
      "Chars:  [(430, 441), (436, 441)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(31, 46), (35, 40), (35, 46), (35, 50), (98, 113), (102, 107), (102, 113)]\n",
      "Title:  Ed Miliband\n",
      "Chars:  [(64, 65), (64, 74), (67, 74)]\n",
      "Title:  Ed Miliband\n",
      "Chars:  [(21, 22), (21, 31), (24, 31), (135, 136), (135, 145), (138, 145)]\n",
      "Title:  Maria Eagle\n",
      "Chars:  [(84, 94)]\n",
      "Title:  European Union\n",
      "Chars:  [(201, 208), (201, 214)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(17, 32), (21, 26), (21, 32)]\n",
      "Title:  European Union\n",
      "Chars:  [(125, 132), (125, 138)]\n",
      "Title:  Ed Miliband\n",
      "Chars:  [(77, 78), (77, 87), (80, 87)]\n",
      "Title:  Hilary Benn\n",
      "Chars:  [(95, 105)]\n",
      "Title:  Keir Starmer\n",
      "Chars:  [(77, 88), (82, 88)]\n",
      "Title:  Sky News\n",
      "Chars:  [(25, 27), (25, 32), (77, 79), (77, 84)]\n",
      "Title:  BBC News\n",
      "Chars:  [(29, 32), (81, 84)]\n",
      "Title:  Dermot Murnaghan\n",
      "Chars:  [(44, 59), (96, 111)]\n",
      "Title:  Saudi Arabia\n",
      "Chars:  [(218, 222)]\n",
      "Title:  Yemen\n",
      "Chars:  [(251, 255)]\n",
      "Title:  Shia Islam\n",
      "Chars:  [(265, 272), (269, 272)]\n",
      "Title:  Houthi movement\n",
      "Chars:  [(274, 280)]\n",
      "Title:  Saudi Arabia\n",
      "Chars:  [(33, 37), (33, 44), (39, 44), (154, 158), (154, 165), (160, 165)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(125, 140), (129, 134), (129, 140)]\n",
      "Title:  Yemen\n",
      "Chars:  [(177, 181)]\n",
      "Title:  Syria\n",
      "Chars:  [(45, 49), (55, 59)]\n",
      "Title:  Bashar al-Assad\n",
      "Chars:  [(71, 76), (71, 84)]\n",
      "Title:  Iran\n",
      "Chars:  [(98, 101), (181, 184)]\n",
      "Title:  Ed Miliband\n",
      "Chars:  [(8, 9), (8, 18), (11, 18), (30, 31), (30, 40), (33, 40)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(10, 15), (10, 18)]\n",
      "Title:  Caroline Flint\n",
      "Chars:  [(20, 33)]\n",
      "Title:  Caroline Flint\n",
      "Chars:  [(60, 73)]\n",
      "Title:  Liberal Democrats (UK)\n",
      "Chars:  [(134, 140)]\n",
      "Title:  Independent politician\n",
      "Chars:  [(145, 154)]\n",
      "Title:  Assassination of Qasem Soleimani\n",
      "Chars:  [(7, 17), (11, 36), (45, 88), (50, 78), (50, 88), (80, 88)]\n",
      "Title:  Qasem Soleimani\n",
      "Chars:  [(22, 36), (28, 36)]\n",
      "Title:  Baghdad\n",
      "Chars:  [(50, 56)]\n",
      "Title:  Baghdad International Airport\n",
      "Chars:  [(50, 70)]\n",
      "Title:  Assassination of Qasem Soleimani\n",
      "Chars:  [(63, 73), (67, 92), (101, 144), (106, 134), (106, 144), (136, 144)]\n",
      "Title:  Qasem Soleimani\n",
      "Chars:  [(78, 92), (84, 92)]\n",
      "Title:  Keir Starmer\n",
      "Chars:  [(21, 32), (26, 32), (142, 153), (147, 153)]\n",
      "Title:  Lisa Nandy\n",
      "Chars:  [(110, 119), (115, 119)]\n",
      "Title:  Labour Party (UK)\n",
      "Chars:  [(168, 183), (172, 177), (172, 183)]\n",
      "Title:  Barry Gardiner\n",
      "Chars:  [(26, 39), (99, 112), (267, 280), (331, 344)]\n",
      "Title:  European Union\n",
      "Chars:  [(49, 66), (53, 60), (53, 66)]\n",
      "Title:  London\n",
      "Chars:  [(85, 90)]\n",
      "Title:  London Borough of Islington\n",
      "Chars:  [(85, 111), (92, 111), (103, 111)]\n",
      "Title:  Islington\n",
      "Chars:  [(43, 51)]\n",
      "Title:  Boris Johnson\n",
      "Chars:  [(38, 50)]\n",
      "Title:  Mount Pleasant Mail Centre\n",
      "Chars:  [(112, 137)]\n",
      "Title:  Royal Mail\n",
      "Chars:  [(127, 137), (169, 178)]\n",
      "Title:  Clerkenwell\n",
      "Chars:  [(188, 198)]\n",
      "Title:  London\n",
      "Chars:  [(15, 20), (180, 188), (183, 188)]\n",
      "Title:  London Borough of Islington\n",
      "Chars:  [(15, 41), (22, 41), (33, 41)]\n",
      "Title:  Boris Johnson\n",
      "Chars:  [(156, 168)]\n",
      "Title:  Boris Johnson\n",
      "Chars:  [(28, 40), (73, 85)]\n",
      "Title:  London\n",
      "Chars:  [(52, 60), (55, 60), (97, 105), (100, 105)]\n",
      "Title:  Mount Pleasant Mail Centre\n",
      "Chars:  [(221, 246)]\n",
      "Title:  Royal Mail\n",
      "Chars:  [(236, 246), (278, 287)]\n",
      "Title:  Emily Davison\n",
      "Chars:  [(14, 26), (91, 103), (156, 168)]\n",
      "Title:  London Borough of Islington\n",
      "Chars:  [(53, 79), (60, 79), (71, 79)]\n",
      "Title:  Christopher Nugee\n",
      "Chars:  [(0, 16), (116, 132)]\n",
      "Title:  London Borough of Tower Hamlets\n",
      "Chars:  [(45, 57), (161, 173)]\n",
      "Title:  Christopher Nugee\n",
      "Chars:  [(0, 16)]\n",
      "Title:  London Borough of Tower Hamlets\n",
      "Chars:  [(45, 57)]\n",
      "Title:  Tony Blair\n",
      "Chars:  [(64, 73)]\n",
      "Title:  Christopher Nugee\n",
      "Chars:  [(44, 60)]\n",
      "Title:  London Borough of Tower Hamlets\n",
      "Chars:  [(89, 101)]\n",
      "Title:  Dame Alice Owen's School\n",
      "Chars:  [(148, 163), (220, 235)]\n",
      "Title:  Dame Alice Owen's School\n",
      "Chars:  [(24, 39), (200, 215)]\n",
      "Title:  Islington\n",
      "Chars:  [(136, 144), (301, 309)]\n",
      "Title:  Chris Woodhead\n",
      "Chars:  [(0, 13), (58, 71)]\n",
      "Title:  Dame Alice Owen's School\n",
      "Chars:  [(52, 67)]\n",
      "{'Cedric Thornberry': 'Person',\n",
      " 'Department of Energy and Climate Change': 'Organisation',\n",
      " 'Islington': 'Location',\n",
      " 'Labour Party (UK)': 'Party',\n",
      " 'London Borough of Islington': 'Location',\n",
      " \"Transport and General Workers' Union\": 'Organisation',\n",
      " 'University of Kent': 'University'}\n",
      "None\n",
      "{'source': 'Emily Thornberry',\n",
      " 'target': 'Labour Party (UK)',\n",
      " 'type': 'per:title'}\n",
      "None\n",
      "{'source': 'Emily Thornberry',\n",
      " 'target': 'Labour Party (UK)',\n",
      " 'type': 'per:employee_of'}\n",
      "None\n",
      "{'source': 'Emily Thornberry',\n",
      " 'target': \"Transport and General Workers' Union\",\n",
      " 'type': 'per:employee_of'}\n",
      "None\n",
      "{'source': 'Emily Thornberry',\n",
      " 'target': 'Cedric Thornberry',\n",
      " 'type': 'per:parents'}\n",
      "None\n",
      "{'source': 'Emily Thornberry',\n",
      " 'target': 'University of Kent',\n",
      " 'type': 'per:schools_attended'}\n",
      "None\n",
      "{'source': 'Emily Thornberry',\n",
      " 'target': 'Department of Energy and Climate Change',\n",
      " 'type': 'per:employee_of'}\n",
      "None\n",
      "{'source': 'Emily Thornberry',\n",
      " 'target': 'Islington',\n",
      " 'type': 'per:cities_of_residence'}\n",
      "None\n",
      "{'source': 'Emily Thornberry',\n",
      " 'target': 'London Borough of Islington',\n",
      " 'type': 'per:cities_of_residence'}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from database import Database\n",
    "import os\n",
    "import pprint\n",
    "import wikipedia\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_coreference_web_trf\")\n",
    "\n",
    "mp_name = 'Emily Thornberry'\n",
    "\n",
    "wiki = wikipedia.page(mp_name)\n",
    "text = wiki.content\n",
    "# prepend wiki text with MP name as in graph as sometimes the wikipedia articles include middle names\n",
    "text = f'{mp_name}. ' + text\n",
    "# only interested in wikipedia content before the references section\n",
    "text = text.split(\"== References ==\")[0]\n",
    "\n",
    "doc = nlp(text)\n",
    "resolve_references(doc)\n",
    "\n",
    "for i, pipe in enumerate(nlp.pipe_names):\n",
    "    print(f\"{i}, {pipe}\")\n",
    "\n",
    "entities, relations = nlp_pipeline(doc, mp_name)\n",
    "print(pprint.pprint(entities))\n",
    "\n",
    "for r in relations:\n",
    "    print(pprint.pprint(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8075fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "driver = Database.init_driver(os.getenv(\"NEO4J_URI\"), os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "filtered_triples = [triple for triple in relations if triple['source'] == 'Chris Bryant']\n",
    "\n",
    "for trip in filtered_triples:\n",
    "    target_name = trip['target']\n",
    "    target_label = entities[target_name]\n",
    "    # stripand capitalise relation type to only include type, so 'EMPLOYEE_OF' instead of 'per:employee_of'\n",
    "    relation_type = trip['type'].split(':')[1].upper()\n",
    "    create_new_rel(driver, 'Chris Bryant', target_label, target_name, relation_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0356a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e975bf0b5e4cb213377fc8c5c8cc667efc04f27b55ac95fed51774df1495ae59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
