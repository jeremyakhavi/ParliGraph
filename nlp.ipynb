{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.load(\"en_coreference_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c92f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "wiki = wikipedia.page('Tulip Siddiq')\n",
    "\n",
    "text = wiki.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce833af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&titles=Chris_Bryant\n",
    "# prepend summary with \"X is an MP\" so that first instance of entity will be given name, not full name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_references(doc):\n",
    "    token_mention_mapper = {}\n",
    "    output_str = \"\"\n",
    "    clusters = [\n",
    "        val for key, val in doc.spans.items() if key.startswith(\"coref_cluster\")\n",
    "    ]\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        first_mention = cluster[0]\n",
    "\n",
    "        for mention_span in list(cluster)[1:]:\n",
    "            token_mention_mapper[mention_span[0].idx] = first_mention.text + mention_span[0].whitespace_\n",
    "            \n",
    "            for token in mention_span[1:]:\n",
    "                token_mention_mapper[token.idx] = \"\"\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.idx in token_mention_mapper:\n",
    "            output_str += token_mention_mapper[token.idx]\n",
    "        else:\n",
    "            output_str += token.text + token.whitespace_\n",
    "    return output_str\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5fde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from string import punctuation\n",
    "import json\n",
    "\n",
    "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
    "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
    "\n",
    "\n",
    "'''\n",
    "pageRankSqThreshold prunes annotations based on their page ranks\n",
    "applyPageRankSqThreshold discards all annotations that have been pruned\n",
    "wikiDataClasses returns wikidata list (concept ID, concept name) for all classes that the concept belongs to\n",
    "maxMentionEntropy ignore highly ambiguous mentions\n",
    "'''\n",
    "\n",
    "def entity_naming(text, lang=\"en\", threshold=0.8):\n",
    "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
    "    # Prepare the URL.\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"lang\", lang),\n",
    "        (\"userKey\", \"bknexcqfanbxjxnubamwxgdnzybwyz\"),\n",
    "        (\"pageRankSqThreshold\", \"%g\" %\n",
    "         threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"support\", \"true\"),\n",
    "        (\"minLinkFrequency\", \"2\"), (\"ranges\", \"false\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "    ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # call wikifier api and read the response\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout=60) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    # output the annotations\n",
    "    results = []\n",
    "    for annotation in response[\"annotations\"]:\n",
    "        # only get desired annotations\n",
    "        if ('wikiDataClasses' in annotation) and (any([e['enLabel'] in ENTITY_TYPES for e in annotation['wikiDataClasses']])):\n",
    "\n",
    "            # Specify entity label\n",
    "            if any([e['enLabel'] in [\"human\", \"person\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'Person'\n",
    "            elif any([e['enLabel'] in [\"company\", \"enterprise\", \"business\", \"organization\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'Organization'\n",
    "            elif any([e['enLabel'] in [\"geographic region\", \"human settlement\", \"geographic entity\", \"territorial entity type\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'Location'\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            results.append({'title': annotation['title'], 'label': label,\n",
    "                            'characters': [(e['chFrom'], e['chTo']) for e in annotation['support']]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8da63f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opennre\n",
    "import nltk\n",
    "import itertools\n",
    "import pprint\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    \"\"\"Removes all punctuation from a string\"\"\"\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "def deduplicate_dict(d):\n",
    "    return [dict(y) for y in set(tuple(x.items()) for x in d)]\n",
    "\n",
    "def nlp_pipeline(doc):\n",
    "    relation_model = opennre.get_model('tacred_bertentity_softmax')\n",
    "    nltk.download('punkt')\n",
    "    relation_threshold = 0.9\n",
    "\n",
    "    entities_dict = {}\n",
    "    relation_dict_list = []\n",
    "    resolved_txt = resolve_references(doc)\n",
    "    for sentence in nltk.sent_tokenize(resolved_txt):\n",
    "        sentence = strip_punctuation(sentence)\n",
    "        entities = entity_naming(sentence)\n",
    "        print(entities)\n",
    "        for entity in entities:\n",
    "            entities_dict[entity['title']] = entity['label']\n",
    "        for p in itertools.permutations(entities, 2):\n",
    "            for source in p[0]['characters']:\n",
    "                for target in p[1]['characters']:\n",
    "                    data = relation_model.infer(\n",
    "                        {'text': sentence, \n",
    "                        'h': {'pos': [source[0], source[1] + 1]}, \n",
    "                        't': {'pos': [target[0], target[1] + 1]}})\n",
    "                    # if confident in infered relationship\n",
    "                    # data = (str:type of relationship, int:confidence)\n",
    "                    if data[1] > relation_threshold and data[0] != 'NA':\n",
    "                        relation_dict_list.append(\n",
    "                            {'source': p[0]['title'], 'target': p[1]['title'], 'type': data[0]})\n",
    "    return entities_dict, deduplicate_dict(relation_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28ede8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feef4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import Database\n",
    "import os\n",
    "\n",
    "# driver = Database.init_driver(os.getenv(\"NEO4J_URI\"), os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "doc = nlp(text)\n",
    "\n",
    "for i, pipe in enumerate(nlp.pipe_names):\n",
    "    print(f\"{i}, {pipe}\")\n",
    "\n",
    "results = nlp_pipeline(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ace87d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unite the Union Organization employee_of\n",
      "The Royal School, Hampstead Organization schools_attended\n",
      "West Hampstead Organization cities_of_residence\n",
      "King's College London Organization schools_attended\n",
      "University College London Organization schools_attended\n",
      "Hampstead Organization cities_of_residence\n",
      "Royal Society Organization employee_of\n",
      "Labour Party (UK) Organization employee_of\n",
      "Royal Society of Arts Organization employee_of\n",
      "Mill Hill School Organization schools_attended\n",
      "Sheikh Rehana Person parents\n",
      "Young Labour (UK) Organization employee_of\n"
     ]
    }
   ],
   "source": [
    "[entities_dict[entity['title']] = entity['label'] for entity in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d00622de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_rel_work(tx, source_name, target_label, target_name, relation_type):\n",
    "    return tx.run(f\"MATCH (m:MP {{name: $source_name}}) \\\n",
    "                  MERGE (t:{target_label} {{name: $target_name}}) \\\n",
    "                  MERGE (m)-[:{relation_type}]->(t) \\\n",
    "                  RETURN m, t\",\n",
    "                  source_name=source_name, target_name=target_name).single()\n",
    "\n",
    "def create_new_rel(driver, source_name, target_label, target_name, relation_type):\n",
    "    session = driver.session()\n",
    "\n",
    "    record = session.execute_write(create_new_rel_work,\n",
    "                                   source_name=source_name, target_label=target_label,\n",
    "                                   target_name=target_name, relation_type=relation_type)\n",
    "    \n",
    "    target = record['t']\n",
    "    print(target)\n",
    "\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e8075fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already intialised\n",
      "<Node element_id='767' labels=frozenset({'Organization'}) properties={'name': 'Unite the Union'}>\n",
      "<Node element_id='768' labels=frozenset({'Organization'}) properties={'name': 'The Royal School, Hampstead'}>\n",
      "<Node element_id='769' labels=frozenset({'Organization'}) properties={'name': 'West Hampstead'}>\n",
      "<Node element_id='770' labels=frozenset({'Organization'}) properties={'name': \"King's College London\"}>\n",
      "<Node element_id='771' labels=frozenset({'Organization'}) properties={'name': 'University College London'}>\n",
      "<Node element_id='772' labels=frozenset({'Organization'}) properties={'name': 'Hampstead'}>\n",
      "<Node element_id='773' labels=frozenset({'Organization'}) properties={'name': 'Royal Society'}>\n",
      "<Node element_id='774' labels=frozenset({'Organization'}) properties={'name': 'Labour Party (UK)'}>\n",
      "<Node element_id='775' labels=frozenset({'Organization'}) properties={'name': 'Royal Society of Arts'}>\n",
      "<Node element_id='776' labels=frozenset({'Organization'}) properties={'name': 'Mill Hill School'}>\n",
      "<Node element_id='777' labels=frozenset({'Person'}) properties={'name': 'Sheikh Rehana'}>\n",
      "<Node element_id='778' labels=frozenset({'Organization'}) properties={'name': 'Young Labour (UK)'}>\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "driver = Database.init_driver(os.getenv(\"NEO4J_URI\"), os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "# TODO: refactor nlp pipeline so that results['entities'] is a dict with key=title and value=label\n",
    "entity_dict = {entity['title']: entity['label'] for entity in results['entities']}\n",
    "\n",
    "filtered_triples = [triple for triple in results['relations'] if triple['source'] == 'Tulip Siddiq']\n",
    "\n",
    "for trip in filtered_triples:\n",
    "    target_name = trip['target']\n",
    "    target_label = entity_dict[target_name]\n",
    "    # stripand capitalise relation type to only include type, so 'EMPLOYEE_OF' instead of 'per:employee_of'\n",
    "    relation_type = trip['type'].split(':')[1].upper()\n",
    "    create_new_rel(driver, 'Tulip Siddiq', target_label, target_name, relation_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
