{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.load(\"en_coreference_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_references(doc):\n",
    "    token_mention_mapper = {}\n",
    "    output_str = \"\"\n",
    "    clusters = [\n",
    "        val for key, val in doc.spans.items() if key.startswith(\"coref_cluster\")\n",
    "    ]\n",
    "    print(clusters)\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        first_mention = cluster[0]\n",
    "        # replace mentions of an entity with the text of the first mention\n",
    "        for mention_span in list(cluster)[1:]:\n",
    "            # key is the index of the mention in the original string\n",
    "            # value is the first mention + the whitespace of the current mention\n",
    "            token_mention_mapper[mention_span[0].idx] = first_mention.text + mention_span[0].whitespace_\n",
    "            \n",
    "            # if there are any other words in the mention, then replace them with the empty string\n",
    "            for token in mention_span[1:]:\n",
    "                token_mention_mapper[token.idx] = \"\"\n",
    "    \n",
    "    # loop through all tokens in original string, if the word/phrase has been identified as an entity\n",
    "    # then replace it with the value given above\n",
    "    for token in doc:\n",
    "        if token.idx in token_mention_mapper:\n",
    "            output_str += token_mention_mapper[token.idx]\n",
    "        else:\n",
    "            output_str += token.text + token.whitespace_\n",
    "    return output_str\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5fde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from string import punctuation\n",
    "import json\n",
    "\n",
    "ENTITY_TYPES = [\"person\",\n",
    "                \"company\", \"business\", \"organization\",\n",
    "                \"city/town\", \"country\", \"geographic region\", \"location\",\n",
    "                \"school\", \"higher education institution\"]\n",
    "\n",
    "'''\n",
    "pageRankSqThreshold prunes annotations based on their page ranks\n",
    "applyPageRankSqThreshold discards all annotations that have been pruned\n",
    "wikiDataClasses returns wikidata list (concept ID, concept name) for all classes that the concept belongs to\n",
    "maxMentionEntropy ignore highly ambiguous mentions\n",
    "'''\n",
    "\n",
    "def entity_naming(text, threshold=0.8):\n",
    "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
    "    # Prepare the URL.\n",
    "    #TODO: move API key to .env file\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"lang\", \"en\"),\n",
    "        (\"userKey\", \"bknexcqfanbxjxnubamwxgdnzybwyz\"),\n",
    "        (\"pageRankSqThreshold\", \"%g\" %\n",
    "         threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"support\", \"true\"),\n",
    "        (\"minLinkFrequency\", \"2\"), (\"ranges\", \"false\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "    ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # call wikifier api and read the response\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout=60) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    # output the annotations\n",
    "    results = []\n",
    "    for annotation in response[\"annotations\"]:\n",
    "        # only get desired annotations\n",
    "        if ('wikiDataClasses' in annotation) and (any([e['enLabel'] in ENTITY_TYPES for e in annotation['wikiDataClasses']])):\n",
    "\n",
    "            # Specify entity label\n",
    "            if any([e['enLabel'] in [\"person\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'Person'\n",
    "            elif any([e['enLabel'] in [\"school\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'School'\n",
    "            elif any([e['enLabel'] in [\"higher education institution\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'University'\n",
    "            elif any([e['enLabel'] in [\"city/town\", \"country\", \"geographic region\", \"location\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'Location'\n",
    "            elif any([e['enLabel'] in [\"company\", \"business\", \"organization\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'Organisation'\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            results.append({'title': annotation['title'], 'label': label,\n",
    "                            'characters': [(e['chFrom'], e['chTo']) for e in annotation['support']]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da63f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opennre\n",
    "import nltk\n",
    "import itertools\n",
    "import pprint\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    \"\"\"Removes all punctuation from a string\"\"\"\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "def deduplicate_dict(d):\n",
    "    return [dict(y) for y in set(tuple(x.items()) for x in d)]\n",
    "\n",
    "def nlp_pipeline(doc):\n",
    "    relation_model = opennre.get_model('tacred_bertentity_softmax')\n",
    "    nltk.download('punkt')\n",
    "    relation_threshold = 0.9\n",
    "\n",
    "    entities_dict = {}\n",
    "    relation_dict_list = []\n",
    "    resolved_txt = resolve_references(doc)\n",
    "    for sentence in nltk.sent_tokenize(resolved_txt):\n",
    "        sentence = strip_punctuation(sentence)\n",
    "        print(sentence)\n",
    "        entities = entity_naming(sentence)\n",
    "        for entity in entities:\n",
    "            entities_dict[entity['title']] = entity['label']\n",
    "        for p in itertools.permutations(entities, 2):\n",
    "            for source in p[0]['characters']:\n",
    "                for target in p[1]['characters']:\n",
    "                    data = relation_model.infer(\n",
    "                        {'text': sentence, \n",
    "                        'h': {'pos': [source[0], source[1] + 1]}, \n",
    "                        't': {'pos': [target[0], target[1] + 1]}})\n",
    "                    # if confident in infered relationship\n",
    "                    # data = (str:type of relationship, int:confidence)\n",
    "                    if data[1] > relation_threshold and data[0] != 'NA':\n",
    "                        relation_dict_list.append(\n",
    "                            {'source': p[0]['title'], 'target': p[1]['title'], 'type': data[0]})\n",
    "    return entities_dict, deduplicate_dict(relation_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_rel_work(tx, source_name, target_label, target_name, relation_type):\n",
    "    return tx.run(f\"MATCH (m:MP {{name: $source_name}}) \\\n",
    "                  MERGE (t:{target_label} {{name: $target_name}}) \\\n",
    "                  MERGE (m)-[:{relation_type}]->(t) \\\n",
    "                  RETURN m, t\",\n",
    "                  source_name=source_name, target_name=target_name).single()\n",
    "\n",
    "def create_new_rel(driver, source_name, target_label, target_name, relation_type):\n",
    "    session = driver.session()\n",
    "\n",
    "    record = session.execute_write(create_new_rel_work,\n",
    "                                   source_name=source_name, target_label=target_label,\n",
    "                                   target_name=target_name, relation_type=relation_type)\n",
    "    \n",
    "    target = record['t']\n",
    "    print(target)\n",
    "\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feef4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import Database\n",
    "import os\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "wiki = wikipedia.page('Emily Thornberry')\n",
    "text = wiki.content\n",
    "\n",
    "# text = (\"\"\"Emily Anne Thornberry (born 27 July 1960) is a British politician who has been Member of Parliament (MP) for Islington South and Finsbury since 2005. Emily Thornberry is member of the Labour Party, she has served as Shadow Attorney General for England and Wales since 2021, and previously from 2011 to 2014. She has also served as Shadow Secretary of State for Foreign and Commonwealth Affairs from 2016 to 2020, Shadow First Secretary of State from 2017 to 2020 and Shadow Secretary of State for International Trade from 2020 to 2021.\"\"\")\n",
    "\n",
    "doc = nlp(text)\n",
    "resolve_references(doc)\n",
    "\n",
    "for i, pipe in enumerate(nlp.pipe_names):\n",
    "    print(f\"{i}, {pipe}\")\n",
    "\n",
    "entities, relations = nlp_pipeline(doc)\n",
    "\n",
    "filtered_triples = [triple for triple in relations if triple['source'] == 'Emily Thornberry']\n",
    "print(filtered_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8075fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "driver = Database.init_driver(os.getenv(\"NEO4J_URI\"), os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "filtered_triples = [triple for triple in relations if triple['source'] == 'Chris Bryant']\n",
    "\n",
    "for trip in filtered_triples:\n",
    "    target_name = trip['target']\n",
    "    target_label = entities[target_name]\n",
    "    # stripand capitalise relation type to only include type, so 'EMPLOYEE_OF' instead of 'per:employee_of'\n",
    "    relation_type = trip['type'].split(':')[1].upper()\n",
    "    create_new_rel(driver, 'Chris Bryant', target_label, target_name, relation_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0356a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e975bf0b5e4cb213377fc8c5c8cc667efc04f27b55ac95fed51774df1495ae59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
