{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.load(\"en_coreference_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c92f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "wiki = wikipedia.page('Tulip Siddiq')\n",
    "\n",
    "text = wiki.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce833af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&titles=Chris_Bryant\n",
    "# prepend summary with \"X is an MP\" so that first instance of entity will be given name, not full name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_references(doc):\n",
    "    token_mention_mapper = {}\n",
    "    output_str = \"\"\n",
    "    clusters = [\n",
    "        val for key, val in doc.spans.items() if key.startswith(\"coref_cluster\")\n",
    "    ]\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        first_mention = cluster[0]\n",
    "\n",
    "        for mention_span in list(cluster)[1:]:\n",
    "            token_mention_mapper[mention_span[0].idx] = first_mention.text + mention_span[0].whitespace_\n",
    "            \n",
    "            for token in mention_span[1:]:\n",
    "                token_mention_mapper[token.idx] = \"\"\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.idx in token_mention_mapper:\n",
    "            output_str += token_mention_mapper[token.idx]\n",
    "        else:\n",
    "            output_str += token.text + token.whitespace_\n",
    "    return output_str\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5fde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from string import punctuation\n",
    "import json\n",
    "\n",
    "ENTITY_TYPES = [\"human\", \"person\", \"company\", \"enterprise\", \"business\", \"geographic region\",\n",
    "                \"human settlement\", \"geographic entity\", \"territorial entity type\", \"organization\"]\n",
    "\n",
    "\n",
    "'''\n",
    "pageRankSqThreshold prunes annotations based on their page ranks\n",
    "applyPageRankSqThreshold discards all annotations that have been pruned\n",
    "wikiDataClasses returns wikidata list (concept ID, concept name) for all classes that the concept belongs to\n",
    "maxMentionEntropy ignore highly ambiguous mentions\n",
    "'''\n",
    "\n",
    "def entity_naming(text, lang=\"en\", threshold=0.8):\n",
    "    \"\"\"Function that fetches entity linking results from wikifier.com API\"\"\"\n",
    "    # Prepare the URL.\n",
    "    data = urllib.parse.urlencode([\n",
    "        (\"text\", text), (\"lang\", lang),\n",
    "        (\"userKey\", \"bknexcqfanbxjxnubamwxgdnzybwyz\"),\n",
    "        (\"pageRankSqThreshold\", \"%g\" %\n",
    "         threshold), (\"applyPageRankSqThreshold\", \"true\"),\n",
    "        (\"support\", \"true\"),\n",
    "        (\"minLinkFrequency\", \"2\"), (\"ranges\", \"false\"),\n",
    "        (\"includeCosines\", \"false\"), (\"maxMentionEntropy\", \"3\")\n",
    "    ])\n",
    "    url = \"http://www.wikifier.org/annotate-article\"\n",
    "    # call wikifier api and read the response\n",
    "    req = urllib.request.Request(url, data=data.encode(\"utf8\"), method=\"POST\")\n",
    "    with urllib.request.urlopen(req, timeout=60) as f:\n",
    "        response = f.read()\n",
    "        response = json.loads(response.decode(\"utf8\"))\n",
    "    # output the annotations\n",
    "    results = []\n",
    "    for annotation in response[\"annotations\"]:\n",
    "        # only get desired annotations\n",
    "        if ('wikiDataClasses' in annotation) and (any([e['enLabel'] in ENTITY_TYPES for e in annotation['wikiDataClasses']])):\n",
    "\n",
    "            # Specify entity label\n",
    "            if any([e['enLabel'] in [\"human\", \"person\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'Person'\n",
    "            elif any([e['enLabel'] in [\"company\", \"enterprise\", \"business\", \"organization\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'Organization'\n",
    "            elif any([e['enLabel'] in [\"geographic region\", \"human settlement\", \"geographic entity\", \"territorial entity type\"] for e in annotation['wikiDataClasses']]):\n",
    "                label = 'Location'\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            results.append({'title': annotation['title'], 'label': label,\n",
    "                            'characters': [(e['chFrom'], e['chTo']) for e in annotation['support']]})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da63f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opennre\n",
    "import nltk\n",
    "import itertools\n",
    "import pprint\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    \"\"\"Removes all punctuation from a string\"\"\"\n",
    "    return ''.join(c for c in s if c not in punctuation)\n",
    "\n",
    "def deduplicate_dict(d):\n",
    "    return [dict(y) for y in set(tuple(x.items()) for x in d)]\n",
    "\n",
    "def nlp_pipeline(doc):\n",
    "    relation_model = opennre.get_model('tacred_bertentity_softmax')\n",
    "    nltk.download('punkt')\n",
    "    relation_threshold = 0.9\n",
    "\n",
    "    entities_dict = {}\n",
    "    relation_dict_list = []\n",
    "    resolved_txt = resolve_references(doc)\n",
    "    for sentence in nltk.sent_tokenize(resolved_txt):\n",
    "        sentence = strip_punctuation(sentence)\n",
    "        entities = entity_naming(sentence)\n",
    "        print(entities)\n",
    "        for entity in entities:\n",
    "            entities_dict[entity['title']] = entity['label']\n",
    "        for p in itertools.permutations(entities, 2):\n",
    "            for source in p[0]['characters']:\n",
    "                for target in p[1]['characters']:\n",
    "                    data = relation_model.infer(\n",
    "                        {'text': sentence, \n",
    "                        'h': {'pos': [source[0], source[1] + 1]}, \n",
    "                        't': {'pos': [target[0], target[1] + 1]}})\n",
    "                    # if confident in infered relationship\n",
    "                    # data = (str:type of relationship, int:confidence)\n",
    "                    if data[1] > relation_threshold and data[0] != 'NA':\n",
    "                        relation_dict_list.append(\n",
    "                            {'source': p[0]['title'], 'target': p[1]['title'], 'type': data[0]})\n",
    "    return entities_dict, deduplicate_dict(relation_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_rel_work(tx, source_name, target_label, target_name, relation_type):\n",
    "    return tx.run(f\"MATCH (m:MP {{name: $source_name}}) \\\n",
    "                  MERGE (t:{target_label} {{name: $target_name}}) \\\n",
    "                  MERGE (m)-[:{relation_type}]->(t) \\\n",
    "                  RETURN m, t\",\n",
    "                  source_name=source_name, target_name=target_name).single()\n",
    "\n",
    "def create_new_rel(driver, source_name, target_label, target_name, relation_type):\n",
    "    session = driver.session()\n",
    "\n",
    "    record = session.execute_write(create_new_rel_work,\n",
    "                                   source_name=source_name, target_label=target_label,\n",
    "                                   target_name=target_name, relation_type=relation_type)\n",
    "    \n",
    "    target = record['t']\n",
    "    print(target)\n",
    "\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feef4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database import Database\n",
    "import os\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for i, pipe in enumerate(nlp.pipe_names):\n",
    "    print(f\"{i}, {pipe}\")\n",
    "\n",
    "entities, relations = nlp_pipeline(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8075fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "driver = Database.init_driver(os.getenv(\"NEO4J_URI\"), os.getenv(\"NEO4J_USERNAME\"), os.getenv(\"NEO4J_PASSWORD\"))\n",
    "\n",
    "filtered_triples = [triple for triple in relations if triple['source'] == 'Chris Bryant']\n",
    "\n",
    "for trip in filtered_triples:\n",
    "    target_name = trip['target']\n",
    "    target_label = entities[target_name]\n",
    "    # stripand capitalise relation type to only include type, so 'EMPLOYEE_OF' instead of 'per:employee_of'\n",
    "    relation_type = trip['type'].split(':')[1].upper()\n",
    "    create_new_rel(driver, 'Chris Bryant', target_label, target_name, relation_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d0be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_key = '4f8fecec31334a60ac1fc93b86878d5f=='\n",
    "company_number = '00214436'\n",
    "url = f'https://api.company-information.service.gov.uk/company/{company_number}/officers'\n",
    "\n",
    "headers = {'Authorization': f'Basic {api_key}'}\n",
    "response = requests.get(url, headers=headers)\n",
    "print(response.text)\n",
    "if response.status_code == 200:\n",
    "    officers = response.json()\n",
    "    print(officers)\n",
    "else:\n",
    "    print(f'Request failed with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0356a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e975bf0b5e4cb213377fc8c5c8cc667efc04f27b55ac95fed51774df1495ae59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
